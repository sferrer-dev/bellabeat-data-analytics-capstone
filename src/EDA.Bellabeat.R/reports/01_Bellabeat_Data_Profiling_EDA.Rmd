---
title: "Bellabeat – Data Profiling and Exploratory Data Analysis"
output:
  html_document:
    theme: cosmo
    css: css/styles.css
    highlight: tango
    toc: true
    toc_depth: 3
    number_sections: true
    df_print: paged
---

# Introduction {.unnumbered}

## Overview {.unnumbered}

This document presents an **automated and fully reproducible R pipeline** designed for data profiling and exploratory data analysis (EDA) on the Bellabeat datasets.\
It establishes a foundation for all subsequent analytical, modelling, and reporting activities.

The pipeline performs a comprehensive assessment of the raw data to validate their **structure**, **quality**, and **temporal coherence** prior to any advanced analysis. It covers **eleven datasets** of `Fitabase Data 3.12.16–4.11.16` sourced from activity-tracking sensors and examines:

-   dataset volume and dimensionality,
-   variable types and inferred classes,
-   missing-value patterns,
-   duplicates and structural inconsistencies,
-   temporal ranges and granularity,
-   key descriptive statistics.

::: tip
The analysis confirms overall dataset stability while identifying notable anomalies — including the **525 duplicates** found in `minuteSleep_merged.csv` — and provides a clear view of the distribution of core metrics such as activity, sleep, heart rate, and calories.
:::

## Technical Keys {.unnumbered}

A central function, **`profile_csv()`**, standardizes all quality checks across datasets: automated type detection, missing-value assessment, cardinality analysis, descriptive statistics, temporal auditing, duplicate identification, and enhanced summaries via **skimr**.

Batch execution through **`purrr::map()`** ensures **homogeneity, reproducibility, and scalability** across the entire processing workflow.

The function **`auto_parse_datetime()`** automatically handles the detection and conversion of temporal columns, ensuring reliable interpretation of dates and timestamps despite the variety of formats present in the raw data. It provides consistent timestamp handling and delivers the uniformity required for all time-based analyses in the pipeline. By centralizing this logic, the pipeline enforces coherent normalization and eliminates ambiguities associated with textual time formats.

::: important
All outputs — HTML and PDF — are automatically generated and consolidated in the project’s **`output/`** directory, ensuring clean separation between computation, reporting, and delivery.
:::

Together, these elements provide a **technical backbone** for subsequent phases of data cleaning, feature engineering, aggregation, and analytical modelling.

------------------------------------------------------------------------

**Using the `render_all.R`**

------------------------------------------------------------------------

To facilitate the automated generation of the HTML of this report, the project includes a dedicated rendering script located at:

```         
scripts/render_all.R
```

This script ensures that output is produced in the directory:

```         
reports/output/
```

------------------------------------------------------------------------

**How to execute the rendering script from RStudio**

------------------------------------------------------------------------

1.  Open the RStudio console.
2.  Run the following command:

``` r
source(here::here("scripts", "render_all.R"))
```

[↑ Return to the beginning](#top)

```{r setup, include=FALSE}
# Global chunk options
knitr::opts_chunk$set(
  echo    = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center"
)

# Packages
library(readr)
library(dplyr)
library(purrr)
library(tidyr)
library(skimr)
library(here)
library(knitr)

# Custom project configuration
source(here("config", "config.R"))
# Custom profiling function
source(here("scripts", "profile_csv.R"))

# Avoid scientific notation by default
options(scipen = 999)
```

```{r batch-profiling, echo=FALSE}
# Batch profiling of all CSV files
# Retrieve the list of all CSV files located in the data directory.
# - pattern = "\\.csv$"   → Only files ending with ".csv"
# - full.names = TRUE     → Return full file paths (needed for reading)
csv_files  <- list.files(
  here(data_dir), 
  pattern = "\\.csv$", 
  full.names = TRUE)
# Apply the profiling function to each CSV file.
# purrr::map() iterates over the list of file paths and returns
# a list of profiling results (one element per CSV).
profiles <- purrr::map(csv_files, profile_csv)
# Name the list using the file_names (without the path)
names(profiles) <- basename(csv_files)
```

```{r render-all-profile-files, echo=FALSE, results='asis', message=FALSE, warning=FALSE}
# Render the profile of each file and dynamically generate the report sections
purrr::iwalk(
  .x = profiles,
  .f = function(p, file_name) {
# Section title and anchor for this file
    name_no_ext <- sub("\\.csv$", "", file_name)
    cat(sprintf("\n\n# File: %s {#%s}\n\n", file_name, name_no_ext))
# Anchor to the other report
    cat(sprintf("\n\n[→ Link to overview report](02_Bellabeat_Files_Overview.html#%s)\n\n", name_no_ext))
# ---- File summary (dimensions) ----
    file_size_fmt <- sprintf("%.2f Mo", p$file_size)
    cat("- **File name :** ", p$file_name, "\n")
    cat("- **File size :** ", file_size_fmt, "\n")
    cat("- **Number of observations :** ", p$n_rows, "\n")
    cat("- **Number of columns :** ", p$n_cols, "\n\n")
    cat("- **Duplicates detecteds :**", if (p$has_duplicates) "Oui" else "Non", "\n\n")
    cat("- **Number of duplicated lines :**", p$n_duplicates, "\n\n")

# 1. Column profile
    cat("## Column profile\n\n")
    print(
      kable(p$col_profile, 
            caption = paste("Variable profile -", file_name)
            )
      )
# 2. Date statistics
    cat("\n\n## Date statistics\n\n")
    print(kable(p$date_stats, 
                caption = paste("Date statistics -", file_name)
                )
          )
# 3. Numerical statistics
    cat("\n\n## Numerical statistics\n\n")
    print(kable(p$num_stats, 
                caption = paste("Numerical statistics -", file_name)
                )
          )
# 4. Skimr summary
    cat("\n\n## Skimr summary\n\n")
    skim_for_report <- p$skim %>%
      dplyr::select(-numeric.hist)  
    print(kable(skim_for_report, 
                caption = paste("Skimr summary -", file_name)
                )
          )
# 5. Raw data overview
    cat("\n\n## Raw data overview\n\n")
    print(
      p$data %>%
        head(10) %>%
        kable(
          caption = paste("Overview (first 10 lines) -", file_name),
          align   = "l"
        )
    )
# 6. Duplicated lines overview    
    if (p$has_duplicates) {
      cat("\n\n## Overview duplicated **\n\n")
      print(kable(p$duplicated |>
                   head(10),
                   caption = paste("Overview (first 10 duplicated lines) -", file_name),
                   align   = "l"
                )
      )
    }
# Link to TOC    
    cat("[↑ Return to the beginning](#top)")
  }
)
```
